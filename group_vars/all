project_name: aws_k8s
env: staging

aws_access_key:  <enter AWS access key>
aws_secret_key:  <enter AWS secret key> 
key_name:  <alias name to the AWS private key used to SSH into the instances> 
aws_region: <e.g., us-east-2>
vpc_id: <e.g., vpc-chdae2>
#official AMI for Ubuntu 16.04 LTS (HVM) Free Tier
ami_id: ami-0782e9ee97725263d
instance_type: t2.micro
my_local_cidr_ip: <enter local computer cidr>

inv_file: "/<>/aws-ansible/inventory"
ansible_remote_user: ubuntu
hoststring: "ansible_ssh_user=ubuntu ansible_ssh_private_key_file=../aws-private.pem"
hostpath: "./Deploy_k8s/hosts"

####### Cluster Details
k8s_cluster_name: "kluster1"
#kubeadm_token: <>   #not required here
flannel_subnet: <>

####### IAM Roles
iam_master_role: "k8s-master-role"
iam_master_policy: "k8s-master-policy"
iam_worker_role: "k8s-worker-role"
iam_worker_policy: "k8s-worker-policy"

######### Instances
k8s_instances:
  -
    index: 0
    krole: "controller"
    public_ip: False
    source_dest_check: True
    iam_role: "{{ iam_master_role }}"
  -
    index: 0
    krole: "worker"
    public_ip: False
    source_dest_check: False
    iam_role: "{{ iam_worker_role }}"
  -
    index: 1
    krole: "worker"
    public_ip: False
    source_dest_check: False
    iam_role: "{{ iam_worker_role }}"
